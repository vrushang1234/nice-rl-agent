{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b287c965-084c-46b9-99dc-c49e358766d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this implementation the input will be vruntime of the process as well as the waiting_time of that process, and the output will be a nice value ranging from -5 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f41925-98a8-4e45-a3cb-3223d112face",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df62980-7a55-4aa0-9a59-8034557cdd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU Activation Function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f34200-e979-4eb1-a9a3-ecd6c2fd4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04f9423d-27ed-4e3b-9b30-2fa80b51a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Activation Function\n",
    "def softmax(x):\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    max_x = np.max(x)\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    den = np.sum(exp_x)\n",
    "    ans = np.zeros_like(exp_x)\n",
    "    for i in range(len(x)):\n",
    "        ans[i] = exp_x[i] / den\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b250bbef-3d59-41f2-8372-90161159eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network forward pass\n",
    "class NeuralNet:\n",
    "    def __init__(self, input_size=2, hidden_size=15, output_size=11, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.W1 = np.random.uniform(-0.01, 0.01, (hidden_size, input_size)).astype(np.float32)\n",
    "        self.b1 = np.zeros((hidden_size, 1), dtype=np.float32)\n",
    "        self.W2 = np.random.uniform(-0.01, 0.01, (output_size, hidden_size)).astype(np.float32)\n",
    "        self.b2 = np.zeros((output_size, 1), dtype=np.float32)\n",
    "        self.baseline = 0.0  \n",
    "    # Forward Propogation   \n",
    "    def forward(self, state):\n",
    "        self.z1 = np.dot(self.W1, state) + self.b1\n",
    "        self.a1 = ReLU(self.z1)\n",
    "        self.z2 = np.dot(self.W2, self.a1) + self.b2\n",
    "        self.probs = softmax(self.z2)\n",
    "        return self.probs\n",
    "\n",
    "    # Select a Random action\n",
    "    # I am doing this for now so that the RL explores all the choices, cruical for training it\n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        action_idx = np.random.choice(len(probs), p=probs.ravel())\n",
    "        nice_value = action_idx - 5\n",
    "        return action_idx, nice_value\n",
    "\n",
    "    # Back propogation to update the Weights and Biases based on the reward received from the RL agent\n",
    "    # I am using a baseline value to calculate the advantage which is then used in back-prop \n",
    "    def update(self, state, action_idx, reward):\n",
    "        self.baseline = 0.9 * self.baseline + 0.1 * reward\n",
    "        advantage = reward - self.baseline\n",
    "\n",
    "        grad_z2 = self.probs.copy()\n",
    "        grad_z2[action_idx] -= 1.0\n",
    "        grad_z2 *= advantage\n",
    "\n",
    "        grad_W2 = np.dot(grad_z2, self.a1.T)\n",
    "        grad_b2 = grad_z2\n",
    "\n",
    "        grad_a1 = np.dot(self.W2.T, grad_z2)\n",
    "        grad_z1 = grad_a1 * relu_deriv(self.z1)\n",
    "\n",
    "        grad_W1 = np.dot(grad_z1, state.T)\n",
    "        grad_b1 = grad_z1\n",
    "\n",
    "        self.W1 += self.lr * grad_W1\n",
    "        self.b1 += self.lr * grad_b1\n",
    "        self.W2 += self.lr * grad_W2\n",
    "        self.b2 += self.lr * grad_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daacb0a7-a40a-4daa-8e1d-962bbd379d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
